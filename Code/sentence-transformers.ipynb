{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.66.1)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for torch>=1.6.0 from https://files.pythonhosted.org/packages/4c/53/f1e58e147df8601c963df4b15045631f7e3d3caa5973bdf4e54a5cf6834e/torch-2.1.0-cp39-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp39-none-macosx_11_0_arm64.whl.metadata (24 kB)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/7d/fd/9c2b3d0200532dc4a6211ef0fcf78c0556a27e3b03800333d4caa32bedc5/torchvision-0.16.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.16.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.26.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/20/0f/51e3ccdc87c25e2e33bf7962249ff8c5ab1d6aed0144fb003348ce8bd352/scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/6f/d9/d10111b008fabab4aea0f98274d3f5db4bd33baadf30c782b6e659ec7708/scipy-1.11.3-cp39-cp39-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scipy-1.11.3-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk (from sentence-transformers)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.18.0)\n",
      "Requirement already satisfied: filelock in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Collecting sympy (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/cd/98/999f0456bdb4124b3d0a7f1d8b6d50979536f5df9856e597580dd9a6d3ff/regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/08/7e/b6e30248aa2c6264383236d993b835876741beb728a9487f5aa12a4c068d/tokenizers-0.14.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/8b/46/91ae7e92277604f8a3457b59d1238ddaffc59e7c2804ac4cb9fd670d58e2/safetensors-0.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting click (from nltk->sentence-transformers)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->sentence-transformers)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.4.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/6a/86/654dc431513cd4417dfcead8102f22bece2d6abf2f584f0e1cc1524f7b94/MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maomao/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.0-cp39-none-macosx_11_0_arm64.whl (59.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp39-cp39-macosx_12_0_arm64.whl (29.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.0-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp39-cp39-macosx_11_0_arm64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.9/425.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.14.1-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4b9236356f8fa35a6fabcf08ae16150d5a85bd4f7d84f1ed9404443174608f1a\n",
      "  Stored in directory: /Users/maomao/Library/Caches/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, mpmath, threadpoolctl, sympy, scipy, safetensors, regex, networkx, MarkupSafe, joblib, click, scikit-learn, nltk, jinja2, huggingface-hub, torch, tokenizers, transformers, torchvision, sentence-transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.18.0\n",
      "    Uninstalling huggingface-hub-0.18.0:\n",
      "      Successfully uninstalled huggingface-hub-0.18.0\n",
      "Successfully installed MarkupSafe-2.1.3 click-8.1.7 huggingface-hub-0.17.3 jinja2-3.1.2 joblib-1.3.2 mpmath-1.3.0 networkx-3.2.1 nltk-3.8.1 regex-2023.10.3 safetensors-0.4.0 scikit-learn-1.3.2 scipy-1.11.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.14.1 torch-2.1.0 torchvision-0.16.0 transformers-4.34.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maomao/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/maomao/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163107363637706862-dalle3-2.png \t\t 1163107363637706862-dalle3-1.png \t\t Score: 0.8626\n",
      "1163107363637706862-dalle3-1.png \t\t 1163107363637706862-dalle3-0.jpeg \t\t Score: 0.6376\n",
      "1163107363637706862-dalle3-2.png \t\t 1163107363637706862-dalle3-0.jpeg \t\t Score: 0.5871\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single list of sentences\n",
    "sentences = ['The image showcases a sleek, modern flashlight with a black metallic finish. The flashlight features pronounced grooves and a textured grip, ensuring a secure hold. Atop the body is a prominent push-button switch, while the front displays a large clear lens, indicating a potentially powerful beam. The flashlight is also equipped with a mounting bracket, suggesting it might be attachable to various surfaces or equipment. The whole design emphasizes both functionality and aesthetics, with attention to detail evident in its construction. The backdrop is a clean white surface, emphasizing the flashlight\\'s design features.',\n",
    "             'This image showcases a modern flashlight with a sleek design. The flashlight features a matte black finish with a textured grip in the middle. The front end of the flashlight displays a clear, large lens that reflects light intricately. Above the textured grip, there\\'s a smooth push-button, likely for power or mode settings. Towards the bottom, there\\'s an adjustable mounting clamp made of metal, designed to attach the flashlight to various surfaces or objects. The overall aesthetic indicates a high-quality and durable product, with attention to design details and functionality.',\n",
    "             'The image displays a black bike light with an elongated cylindrical shape. On the top of the light, there\\'s a round button, and the front part features a clear lens through which the light would shine. Surrounding the lens is a transparent checkered pattern which might aid in light dispersion. The bike light is attached to a sturdy black mounting bracket, designed to be clamped onto a handlebar or similar structure. There\\'s a visible cable extending from the rear of the light. The brand word \"Philips\" is engraved on the mounting bracket. The image appears to be a product photograph, showcasing the item against a white background.'\n",
    "            ]\n",
    "\n",
    "images_represented_sentences = ['1163107363637706862-dalle3-2.png', '1163107363637706862-dalle3-1.png', '1163107363637706862-dalle3-0.jpeg']\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs[0:10]:\n",
    "    i, j = pair['index']\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(images_represented_sentences[i], images_represented_sentences[j], pair['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': [0, 1], 'score': tensor(0.8659)}, {'index': [0, 2], 'score': tensor(0.8682)}, {'index': [0, 3], 'score': tensor(0.7605)}, {'index': [0, 4], 'score': tensor(0.8108)}, {'index': [0, 5], 'score': tensor(0.8052)}, {'index': [0, 6], 'score': tensor(0.8401)}, {'index': [0, 7], 'score': tensor(0.7548)}, {'index': [0, 8], 'score': tensor(0.8556)}, {'index': [1, 2], 'score': tensor(0.8997)}, {'index': [1, 3], 'score': tensor(0.8583)}, {'index': [1, 4], 'score': tensor(0.8694)}, {'index': [1, 5], 'score': tensor(0.8978)}, {'index': [1, 6], 'score': tensor(0.8917)}, {'index': [1, 7], 'score': tensor(0.8727)}, {'index': [1, 8], 'score': tensor(0.8622)}, {'index': [2, 3], 'score': tensor(0.8130)}, {'index': [2, 4], 'score': tensor(0.8577)}, {'index': [2, 5], 'score': tensor(0.8890)}, {'index': [2, 6], 'score': tensor(0.8310)}, {'index': [2, 7], 'score': tensor(0.8323)}, {'index': [2, 8], 'score': tensor(0.9035)}, {'index': [3, 4], 'score': tensor(0.9333)}, {'index': [3, 5], 'score': tensor(0.8711)}, {'index': [3, 6], 'score': tensor(0.8310)}, {'index': [3, 7], 'score': tensor(0.8821)}, {'index': [3, 8], 'score': tensor(0.7432)}, {'index': [4, 5], 'score': tensor(0.9131)}, {'index': [4, 6], 'score': tensor(0.8577)}, {'index': [4, 7], 'score': tensor(0.8829)}, {'index': [4, 8], 'score': tensor(0.7936)}, {'index': [5, 6], 'score': tensor(0.8814)}, {'index': [5, 7], 'score': tensor(0.8738)}, {'index': [5, 8], 'score': tensor(0.8720)}, {'index': [6, 7], 'score': tensor(0.8341)}, {'index': [6, 8], 'score': tensor(0.7971)}, {'index': [7, 8], 'score': tensor(0.7955)}]\n",
      "pair(4, 5): 0.933296263217926\n",
      "pair(5, 6): 0.9130679368972778\n",
      "pair(3, 9): 0.90352863073349\n",
      "pair(2, 3): 0.8996773958206177\n",
      "pair(2, 6): 0.8978211283683777\n",
      "pair(2, 7): 0.8916803598403931\n",
      "pair(3, 6): 0.8890079259872437\n",
      "pair(5, 8): 0.8829329013824463\n",
      "pair(4, 8): 0.8821008205413818\n",
      "pair(6, 7): 0.8814231157302856\n",
      "pair(6, 8): 0.873833179473877\n",
      "pair(2, 8): 0.8727237582206726\n",
      "pair(6, 9): 0.8719511032104492\n",
      "pair(4, 6): 0.8711414337158203\n",
      "pair(2, 5): 0.8693932890892029\n",
      "pair(1, 3): 0.8682497143745422\n",
      "pair(1, 2): 0.8659034967422485\n",
      "pair(2, 9): 0.8621756434440613\n",
      "pair(2, 4): 0.8582561016082764\n",
      "pair(5, 7): 0.8576880097389221\n",
      "pair(3, 5): 0.8576679825782776\n",
      "pair(1, 9): 0.855555534362793\n",
      "pair(1, 7): 0.8400592803955078\n",
      "pair(7, 8): 0.8340519666671753\n",
      "pair(3, 8): 0.8323116302490234\n",
      "pair(3, 7): 0.8310363292694092\n",
      "pair(4, 7): 0.83099365234375\n",
      "pair(3, 4): 0.8129801750183105\n",
      "pair(1, 5): 0.8108152151107788\n",
      "pair(1, 6): 0.8051769137382507\n",
      "pair(7, 9): 0.7971118688583374\n",
      "pair(8, 9): 0.7955432534217834\n",
      "pair(5, 9): 0.7935530543327332\n",
      "pair(1, 4): 0.7605247497558594\n",
      "pair(1, 8): 0.7547886371612549\n",
      "pair(4, 9): 0.7432466745376587\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single list of sentences\n",
    "sentences = ['The image features a highly detailed and stylized portrait of an elderly woman. Her face is deeply lined with wrinkles, reflecting her advanced age. She wears large, round spectacles which sit on the bridge of her nose, and through which her eyes are visible, showing a sense of wisdom and experience. The glasses seem to reflect some light, indicating they might be set against a lighter background or an external light source. She is wearing a headscarf that is wrapped around her head and shoulders, with a texture that somewhat resembles a honeycomb pattern, suggesting a soft, woven fabric. The scarf is rendered in warm tones of yellow and gold, contrasting with a cooler, bluish shade at the edges. The background of the image is soft and appears to be intentionally blurred or washed out, directing the focus entirely on the woman. The art style has a watercolor feel, with colors bleeding into one another, and the woman\\'s visage is central and prominent, conveying a strong sense of character and presence.',\n",
    "             'The image depicts a close-up portrait of an elderly woman with a kind and gentle expression. She has white hair, visible wrinkles, and is wearing large round glasses with a thin, gold-toned frame. The woman is also wearing what appears to be a headscarf or shawl with yellow and blue tones that drapes around her head and shoulders. Her eyes are light-colored, and she seems to be looking directly at the viewer with a soft smile. The background is abstract with muted colors that do not distract from the subject. The artwork has a realistic quality, possibly rendered in a medium that resembles watercolor or a similar fluid painting style, with the colors beautifully blending into each other.',\n",
    "             'The image is a close-up portrait of an elderly woman. She has visible wrinkles and signs of aging, which are detailed, suggesting the wisdom and life experiences she may have acquired over the years. Her hair appears to be gray and neatly tied back, and she wears round glasses that sit in front of her blue eyes, reflecting a gentle gaze. The woman has a soft smile, indicating a serene demeanor. The portrait is rich with texture that resembles watercolor, giving it an artistic and warm feel. Her clothing is not fully visible, but what can be seen suggests a simple, classic style, with a hint of a pattern that might suggest a shawl or a scarf draped over her shoulders. The background is indistinct, allowing the focus to remain on her expressive face.',\n",
    "             'The image is a watercolor painting of an elderly woman with white hair. She is wearing round glasses and has a gentle smile on her face. The woman has stud earrings and appears to be wearing a scarf with warm tones of orange and brown. The background is abstract, consisting of cool-toned watercolor washes that suggest a vague sense of space. The painting style captures the textures of the woman\\'s wrinkles and the softness of her expression, characteristic of skilled watercolor techniques that allow for both detail and fluidity.',\n",
    "             'The image is a watercolor painting of an elderly woman with a gentle expression. She has visible wrinkles and grey hair, suggesting her advanced age. Her eyes, behind a pair of round glasses, appear kind and reflective. She is wearing a light scarf or shawl over her head and shoulders, with its folds softly draped around her. The color palette is neutral and warm, conveying a sense of calmness and wisdom. The brush strokes and watercolor bleed effects add to the artistic, timeless quality of the portrait.',\n",
    "             'The image is a watercolor portrait of an elderly woman with a gentle smile. She has deep-set, wise eyes, accentuated by large, round glasses with a light brown frame. Her skin is rich with history, mapped with wrinkles that speak of a long life lived. She\\'s wrapped in a shawl or headscarf with subtle floral patterns, and the soft, warm hues of the painting give the impression of a tender, compassionate soul. The artwork beautifully captures the intricate play of light and shadow, creating a realistic and expressive portrayal that conveys both strength and tenderness.',\n",
    "             'The image depicts a close-up portrait of an elderly woman with a joyful expression. She has deep wrinkles and smile lines, which suggest a long life lived with much laughter. Her eyes are magnified behind round spectacles, which adds to the warmth of her gaze. She wears a headscarf that wraps around her head, partially obscuring her hair. The artwork has the appearance of a watercolor painting, characterized by fluid brushstrokes and a harmonious blend of colors, primarily in earthy tones. The level of detail in rendering the textures of her skin and the fabric of her headscarf is quite remarkable, giving the image a rich and tactile quality.',\n",
    "             'The image you\\'ve uploaded appears to be a watercolor painting of an elderly woman smiling. She is wearing round glasses and a headscarf or shawl in hues of yellow and brown with a checkered pattern. Her face has many wrinkles, indicating age, and her hair is white, suggesting that she is quite old. The watercolor technique gives the portrait a fluid and somewhat translucent quality, with colors bleeding into one another, which is characteristic of this medium. The background is minimal, allowing the focus to remain on the woman\\'s expressive face.',\n",
    "             'The image is a close-up portrait of an elderly woman with a joyful expression. She has deep smile lines and wrinkles, which suggest a life rich with experiences. Her eyes, magnified slightly by the round glasses she wears, appear bright and expressive, reflecting a warm and kind personality. The glasses have a classic design, with a thin frame that seems to be a light, possibly golden color. She is wearing what looks like a knitted garment, possibly a shawl or scarf, that adds to the warmth of the image. The overall impression is one of happiness, wisdom, and the kind of contentment that comes with age. The lighting and resolution are high, indicating that this is either a high-quality photograph or a detailed painting, capturing the intricate details of her face and the textures of her skin and clothing.']\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "# print(pairs)\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs:\n",
    "    i, j = pair['index']\n",
    "    # print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))\n",
    "    score = pair['score']\n",
    "    print(f'pair({i+1}, {j+1}): {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair(3, 6): 0.9355354905128479\n",
      "pair(2, 4): 0.9197893142700195\n",
      "pair(1, 2): 0.8999609351158142\n",
      "pair(3, 7): 0.899485170841217\n",
      "pair(3, 8): 0.8981077075004578\n",
      "pair(6, 8): 0.8960297703742981\n",
      "pair(4, 5): 0.8926772475242615\n",
      "pair(6, 7): 0.8919727802276611\n",
      "pair(4, 7): 0.8910467028617859\n",
      "pair(5, 7): 0.8867325782775879\n",
      "pair(2, 3): 0.8853703737258911\n",
      "pair(3, 4): 0.8852168321609497\n",
      "pair(3, 5): 0.8813087344169617\n",
      "pair(1, 4): 0.8766710758209229\n",
      "pair(1, 3): 0.8747601509094238\n",
      "pair(5, 6): 0.8718196153640747\n",
      "pair(2, 6): 0.8718121647834778\n",
      "pair(4, 6): 0.8711798787117004\n",
      "pair(1, 8): 0.8635278344154358\n",
      "pair(4, 8): 0.8633603453636169\n",
      "pair(2, 5): 0.8616672158241272\n",
      "pair(7, 9): 0.8610360622406006\n",
      "pair(2, 7): 0.8556584119796753\n",
      "pair(4, 9): 0.8543829917907715\n",
      "pair(2, 8): 0.8537253141403198\n",
      "pair(1, 5): 0.8507130146026611\n",
      "pair(5, 8): 0.8487952947616577\n",
      "pair(1, 9): 0.8467739224433899\n",
      "pair(1, 6): 0.8423457741737366\n",
      "pair(1, 7): 0.8391168117523193\n",
      "pair(7, 8): 0.8364249467849731\n",
      "pair(5, 9): 0.8185986876487732\n",
      "pair(3, 9): 0.8058867454528809\n",
      "pair(2, 9): 0.7836478352546692\n",
      "pair(8, 9): 0.7809042930603027\n",
      "pair(6, 9): 0.7771084308624268\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Single list of sentences\n",
    "sentences = ['The image is a close-up of an elderly woman\\'s face. She appears to be smiling gently, with her eyes crinkled in a way that suggests warmth and friendliness. The woman is wearing large, round eyeglasses with a slight tint, which adds to her wise and kind demeanor. Her skin shows signs of aging with deep wrinkles and lines, imparting a sense of a life fully lived. She has gray hair, part of which is swept back, and she is wearing an earring visible on her left ear, adding a touch of elegance. The artwork has the soft, textured look of a watercolor painting, giving it an artistic and expressive quality.',\n",
    "             'The image is a portrait of an elderly woman with a warm and gentle expression. She wears large, round, brown-rimmed glasses that accentuate the blue of her eyes. The woman\\'s face is deeply lined, telling a story of many years lived, and her skin has the textured appearance characteristic of advanced age. Her white hair is barely visible, and she is wrapped in a soft, textured yellow shawl or headscarf that drapes comfortably around her head and shoulders. The background is plain and watercolor-like, with a light wash of blue that fades into white, highlighting the subject without distraction. The artwork has a realistic yet softly rendered quality, likely done in watercolor or a similar medium, giving it an almost ethereal feel.',\n",
    "             'The image is a watercolor portrait of an elderly woman with a kind and gentle expression. She has silver-gray hair, visible under a light brown headscarf. Her eyes are light-colored, and she is wearing round, gold-rimmed glasses. The woman has a slight smile, and her face is marked with wrinkles that suggest a life rich with experience. The painting style captures the delicate textures of her skin and the soft fabric of her scarf with fluid watercolor washes and detailed brushwork. The background is minimal, with loose watercolor marks, focusing the attention on the woman’s face.',\n",
    "             'The image is a close-up portrait of an elderly woman with a warm smile. She has silver-white hair and is wearing large, round glasses with a gold frame. The glasses rest on her prominent, cheerful cheeks, and her eyes have a kind sparkle that suggests wisdom and gentleness. The woman is wearing a yellow headscarf with hints of orange and red patterns, which drapes softly around her head and shoulders. The background appears to be a soft, undefined blue, which contrasts gently with the warm tones of her attire. The overall impression is of a kindly grandmotherly figure, and the artwork appears to be rendered in watercolor, giving it a fluid and soft appearance.',\n",
    "             'The image depicts a close-up portrait of an elderly woman with a serene expression. She has visible wrinkles and grey hair, suggesting wisdom and a life well-lived. Her eyes are gentle, framed by round glasses, and she appears to be gazing calmly towards the viewer. She is wearing a headscarf with shades of orange and brown, which adds warmth to her visage. The medium appears to be watercolor, given the fluid transitions of color and the presence of wash effects throughout the piece. This artwork conveys a sense of peaceful aging and a quiet dignity.',\n",
    "             'The image is a detailed watercolor portrait of an elderly woman with visible wrinkles and a kind expression. She wears round, gold-rimmed glasses and a headscarf adorned with subtle floral patterns and hues of purple and brown, suggesting a touch of elegance and history. Her gaze is directed slightly upwards, and there\\'s a sense of wisdom and reflection in her eyes. The painting captures the texture of her skin and the soft, flowing fabric of her scarf with a high level of realism. The background and her clothing blend with splashes and dribbles of paint, giving the artwork a dynamic and organic feel. The brush and palette suggest that the image itself may be part of an artist\\'s workspace.',\n",
    "             'The image depicts a close-up portrait of an elderly woman. She has a warm, gentle smile and is wearing round spectacles. Her face is rich with character, featuring deep wrinkles and creases that speak to her age and the life she has lived. The woman is wearing a headscarf with a floral pattern, suggesting a sense of style and perhaps cultural significance. The artwork has a very detailed and realistic watercolor aesthetic, blending soft washes of color with precise, fine details, particularly around the eyes and the texture of the skin. The background is abstract, done in washes of neutral tones that do not distract from the central figure.',\n",
    "             'The image is a painting that features a close-up portrait of an elderly woman with a joyful expression. She has glasses with round, thick frames that sit on her nose, magnifying her kind eyes, which crinkle with her wide, warm smile. Her hair is gray, and she wears a yellow scarf draped over her head and around her neck, giving her a look of elegance and grace. The scarf has intricate details and patterns, with hues of mustard and gold that complement her cheerful demeanor. The background of the painting appears to be abstract, with watercolor washes and splatters in neutral and earthy tones, suggesting a blend of sophistication and artistic freedom. The artwork has a comforting and endearing quality, capturing the beauty of old age with respect and affection.',\n",
    "             'The image shows a close-up of an elderly woman\\'s face. She has a warm and friendly expression with a gentle smile. Her eyes are framed by round, gold-rimmed glasses, and her skin has deep, natural wrinkles that speak to her advanced age. The woman wears a headscarf with a subtle pattern, which suggests she might be from a culture or region where this is customary. The details in the image are very fine, indicating that the picture is either a high-resolution photograph or a detailed painting. Her eyes have a sparkle that conveys kindness and wisdom acquired over many years.']\n",
    "\n",
    "#Compute embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities for each sentence with each other sentence\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for pair in pairs:\n",
    "    i, j = pair['index']\n",
    "    # print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))\n",
    "    score = pair['score']\n",
    "    print(f'pair({i+1}, {j+1}): {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_files1 = [\n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence1_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence2_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence3_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence4_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence5_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence6_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence7_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence8_img2.png', \n",
    "    '/Users/maomao/Documents/GitHub/CS523Project/Results/elder_woman/DALLE_sentence9_img2.png'\n",
    "]\n",
    "\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\begin{tabular}{cccc}\n",
    "\\hline\n",
    "\\textbf{Pair} & \\textbf{GPT Score} & \\textbf{Mean Cosine Score} & \\textbf{Difference} \\\\\n",
    "\\hline\n",
    "(1, 2) & 0.90 & 0.90 & 0.00 \\\\\n",
    "(1, 3) & 0.85 & 0.85 & 0.00 \\\\\n",
    "(1, 4) & 0.80 & 0.80 & 0.00 \\\\\n",
    "(1, 7) & 0.75 & 0.75 & 0.00 \\\\\n",
    "(2, 3) & 0.90 & 0.90 & 0.00 \\\\\n",
    "(2, 4) & 0.80 & 0.80 & 0.00 \\\\\n",
    "(2, 7) & 0.75 & 0.75 & 0.00 \\\\\n",
    "(3, 4) & 0.85 & 0.85 & 0.00 \\\\\n",
    "(3, 7) & 0.80 & 0.80 & 0.00 \\\\\n",
    "(4, 7) & 0.85 & 0.85 & 0.00 \\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Comparison of cosine scores between sentence pairs}\n",
    "\\label{tab:cosine_scores}\n",
    "\\end{table}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maomao/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/maomao/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 955kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 192kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 36.2MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 2.98MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 601kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 6.55MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:04<00:00, 21.5MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 25.3kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 139kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.59MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 253kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 7.18MB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 9.47MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 345kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173412e-02 -4.28515561e-02 -1.56286638e-02  1.40536716e-02\n",
      "  3.95537838e-02  1.21796325e-01  2.94333566e-02 -3.17523777e-02\n",
      "  3.54959853e-02 -7.93140084e-02  1.75878648e-02 -4.04369496e-02\n",
      "  4.97259758e-02  2.54912935e-02 -7.18701407e-02  8.14969018e-02\n",
      "  1.47065788e-03  4.79627140e-02 -4.50335741e-02 -9.92174372e-02\n",
      " -2.81769671e-02  6.45045936e-02  4.44670580e-02 -4.76217084e-02\n",
      " -3.52951698e-02  4.38671932e-02 -5.28565906e-02  4.32968431e-04\n",
      "  1.01921357e-01  1.64072681e-02  3.26996557e-02 -3.45986336e-02\n",
      "  1.21339764e-02  7.94870928e-02  4.58342815e-03  1.57777481e-02\n",
      " -9.68207326e-03  2.87626460e-02 -5.05806729e-02 -1.55793978e-02\n",
      " -2.87907012e-02 -9.62286908e-03  3.15556787e-02  2.27349121e-02\n",
      "  8.71449262e-02 -3.85027193e-02 -8.84718373e-02 -8.75500031e-03\n",
      " -2.12342720e-02  2.08923463e-02 -9.02077556e-02 -5.25732413e-02\n",
      " -1.05639296e-02  2.88311280e-02 -1.61455348e-02  6.17838372e-03\n",
      " -1.23234773e-02 -1.07337767e-02  2.83353962e-02 -5.28567806e-02\n",
      " -3.58617865e-02 -5.97989373e-02 -1.09055322e-02  2.91567016e-02\n",
      "  7.97979683e-02 -3.27867456e-04  6.83497125e-03  1.32718533e-02\n",
      " -4.24619131e-02  1.87656395e-02 -9.89234224e-02  2.09049806e-02\n",
      " -8.69606212e-02 -1.50152063e-02 -4.86202277e-02  8.04413930e-02\n",
      " -3.67699773e-03 -6.65044859e-02  1.14556767e-01 -3.04229781e-02\n",
      "  2.96631977e-02 -2.80695260e-02  4.64989580e-02 -2.25513466e-02\n",
      "  8.54223445e-02  3.15446854e-02  7.34541789e-02 -2.21862048e-02\n",
      " -5.29678501e-02  1.27129871e-02 -5.27340025e-02 -1.06188692e-01\n",
      "  7.04731122e-02  2.76737139e-02 -8.05531815e-02  2.39649415e-02\n",
      " -2.65124198e-02 -2.17330754e-02  4.35275771e-02  4.84711677e-02\n",
      " -2.37066988e-02  2.85767652e-02  1.11846112e-01 -6.34936318e-02\n",
      " -1.58318635e-02 -2.26169508e-02 -1.31027838e-02 -1.62066135e-03\n",
      " -3.60928662e-02 -9.78297815e-02 -4.67728637e-02  1.76271368e-02\n",
      " -3.97492237e-02 -1.76355039e-04  3.39627452e-02 -2.09633820e-02\n",
      "  6.33667642e-03 -2.59410590e-02  8.10410306e-02  6.14393763e-02\n",
      " -5.44597255e-03  6.48275465e-02 -1.16844021e-01  2.36860793e-02\n",
      " -1.32058291e-02 -1.12476446e-01  1.90048888e-02 -1.74661173e-34\n",
      "  5.58950529e-02  1.94244720e-02  4.65438925e-02  5.18646091e-02\n",
      "  3.89390141e-02  3.40541191e-02 -4.32113968e-02  7.90636837e-02\n",
      " -9.79529917e-02 -1.27441213e-02 -2.91870609e-02  1.02052204e-02\n",
      "  1.88116115e-02  1.08942591e-01  6.63464889e-02 -5.35295382e-02\n",
      " -3.29228453e-02  4.69827540e-02  2.28883438e-02  2.74114404e-02\n",
      " -2.91983429e-02  3.12706977e-02 -2.22851150e-02 -1.02282196e-01\n",
      " -2.79116761e-02  1.13793360e-02  9.06307623e-02 -4.75414097e-02\n",
      " -1.00718945e-01 -1.23231979e-02 -7.96928406e-02 -1.44635933e-02\n",
      " -7.76401237e-02 -7.66923092e-03  9.73954145e-03  2.24204920e-02\n",
      "  7.77268484e-02 -3.17161903e-03  2.11538058e-02 -3.30394171e-02\n",
      "  9.55246948e-03 -3.73012163e-02  2.61361003e-02 -9.79087595e-03\n",
      " -6.31505176e-02  5.77437412e-03 -3.80031206e-02  1.29684275e-02\n",
      " -1.82499327e-02 -1.56283081e-02 -1.23363710e-03  5.55579066e-02\n",
      "  1.13095375e-04 -5.61256148e-02  7.40164444e-02  1.84452180e-02\n",
      " -2.66368724e-02  1.31951692e-02  7.50086606e-02 -2.46796515e-02\n",
      " -3.24006379e-02 -1.57675128e-02 -8.03521927e-03 -5.61322691e-03\n",
      "  1.05687622e-02  3.26163298e-03 -3.91990021e-02 -9.38677490e-02\n",
      "  1.14227228e-01  6.57305196e-02 -4.72634137e-02  1.45087885e-02\n",
      " -3.54490019e-02 -3.37761939e-02 -5.15505634e-02 -3.80999781e-03\n",
      " -5.15035950e-02 -5.93429208e-02 -1.69413805e-03  7.42107108e-02\n",
      " -4.20092158e-02 -7.19975010e-02  3.17250118e-02 -1.66303590e-02\n",
      "  3.96978017e-03 -6.52750582e-02  2.77390722e-02 -7.51649141e-02\n",
      "  2.27455776e-02 -3.91368382e-02  1.54315364e-02 -5.54908030e-02\n",
      "  1.23319197e-02 -2.59520821e-02  6.66423738e-02 -6.91259601e-34\n",
      "  3.31628993e-02  8.47928748e-02 -6.65583089e-02  3.33542041e-02\n",
      "  4.71611694e-03  1.35361878e-02 -5.38694188e-02  9.20694470e-02\n",
      " -2.96876766e-02  3.16219740e-02 -2.37497352e-02  1.98770612e-02\n",
      "  1.03446186e-01 -9.06947851e-02  6.30628597e-03  1.42886778e-02\n",
      "  1.19294301e-02  6.43722806e-03  4.20104377e-02  1.25343995e-02\n",
      "  3.93019691e-02  5.35691306e-02 -4.30749841e-02  6.10432960e-02\n",
      " -5.39559551e-05  6.91682845e-02  1.05519937e-02  1.22111496e-02\n",
      " -7.23184720e-02  2.50470322e-02 -5.18370718e-02 -4.36562710e-02\n",
      " -6.71818852e-02  1.34828798e-02 -7.25889206e-02  7.04169506e-03\n",
      "  6.58939928e-02  1.08994702e-02 -2.60014343e-03  5.49969114e-02\n",
      "  5.06966636e-02  3.27947885e-02 -6.68833330e-02  6.45557344e-02\n",
      " -2.52075680e-02 -2.92572696e-02 -1.16696693e-01  3.24065238e-02\n",
      "  5.85858561e-02 -3.51756737e-02 -7.15240166e-02  2.24935934e-02\n",
      " -1.00786664e-01 -4.74544875e-02 -7.61962458e-02 -5.87166585e-02\n",
      "  4.21137922e-02 -7.47213736e-02  1.98468380e-02 -3.36505054e-03\n",
      " -5.29736392e-02  2.74730120e-02  3.45736481e-02 -6.11845814e-02\n",
      "  1.06364824e-01 -9.64119807e-02 -4.55944687e-02  1.51489712e-02\n",
      " -5.13532804e-03 -6.64447173e-02  4.31721509e-02 -1.10406140e-02\n",
      " -9.80252214e-03  7.53782690e-02 -1.49570405e-02 -4.80208620e-02\n",
      "  5.80726638e-02 -2.43897084e-02 -2.23136917e-02 -4.36992273e-02\n",
      "  5.12053743e-02 -3.28625925e-02  1.08763330e-01  6.08926937e-02\n",
      "  3.30791366e-03  5.53819872e-02  8.43201727e-02  1.27087058e-02\n",
      "  3.84466276e-02  6.52325526e-02 -2.94683743e-02  5.08005470e-02\n",
      " -2.09347773e-02  1.46135673e-01  2.25561354e-02 -1.77227726e-08\n",
      " -5.02673015e-02 -2.79179803e-04 -1.00328609e-01  2.42810864e-02\n",
      " -7.54043311e-02 -3.79140042e-02  3.96050476e-02  3.10080275e-02\n",
      " -9.05700866e-03 -6.50411770e-02  4.05452810e-02  4.83390205e-02\n",
      " -4.56962362e-02  4.76006325e-03  2.64365156e-03  9.35614482e-02\n",
      " -4.02599536e-02  3.27402167e-02  1.18298000e-02  5.54344393e-02\n",
      "  1.48052290e-01  7.21189231e-02  2.77018145e-04  1.68651249e-02\n",
      "  8.34879931e-03 -8.76155030e-03 -1.33649604e-02  6.14237748e-02\n",
      "  1.57168265e-02  6.94961250e-02  1.08620813e-02  6.08018041e-02\n",
      " -5.33421747e-02 -3.47924121e-02 -3.36271971e-02  6.93906695e-02\n",
      "  1.22987190e-02 -1.45237371e-01 -2.06974312e-03 -4.61132750e-02\n",
      "  3.72745213e-03 -5.59359509e-03 -1.00659817e-01 -4.45953272e-02\n",
      "  5.40922023e-02  4.98887990e-03  1.49534661e-02 -8.26059356e-02\n",
      "  6.26630560e-02 -5.01906639e-03 -4.81857955e-02 -3.53990905e-02\n",
      "  9.03388578e-03 -2.42338106e-02  5.66267222e-02  2.51528863e-02\n",
      " -1.70709491e-02 -1.24780564e-02  3.19518447e-02  1.38420919e-02\n",
      " -1.55814691e-02  1.00178219e-01  1.23657189e-01 -4.22967039e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 5.64524941e-02  5.50023876e-02  3.13796103e-02  3.39485332e-02\n",
      " -3.54246870e-02  8.34667832e-02  9.88801047e-02  7.27543002e-03\n",
      " -6.68660644e-03 -7.65811745e-03  7.93738216e-02  7.39662384e-04\n",
      "  1.49291363e-02 -1.51046896e-02  3.67674455e-02  4.78743277e-02\n",
      " -4.81969640e-02 -3.76053117e-02 -4.60277759e-02 -8.89815986e-02\n",
      "  1.20228149e-01  1.30663216e-01 -3.73935625e-02  2.47851713e-03\n",
      "  2.55821575e-03  7.25814477e-02 -6.80436268e-02 -5.24696670e-02\n",
      "  4.90234233e-02  2.99563371e-02 -5.84429875e-02 -2.02263072e-02\n",
      "  2.08821725e-02  9.76691619e-02  3.52390893e-02  3.91140804e-02\n",
      "  1.05668595e-02  1.56234577e-03 -1.30822593e-02  8.52903537e-03\n",
      " -4.84092953e-03 -2.03767102e-02 -2.71801073e-02  2.83307880e-02\n",
      "  3.66017744e-02  2.51275729e-02 -9.90861803e-02  1.15626687e-02\n",
      " -3.60380597e-02 -7.23784342e-02 -1.12670086e-01  1.12942187e-02\n",
      " -3.86397205e-02  4.67385948e-02 -2.88460646e-02  2.26703752e-02\n",
      " -8.52406956e-03  3.32814902e-02 -1.06580788e-03 -7.09745213e-02\n",
      " -6.31169975e-02 -5.72186932e-02 -6.16026632e-02  5.47146611e-02\n",
      "  1.18317697e-02 -4.66261059e-02  2.56959908e-02 -7.07418891e-03\n",
      " -5.73843047e-02  4.12839018e-02 -5.91503493e-02  5.89021482e-02\n",
      " -4.41697575e-02  4.65080887e-02 -3.15814540e-02  5.58312535e-02\n",
      "  5.54579459e-02 -5.96533865e-02  4.06407304e-02  4.83762752e-03\n",
      " -4.96767871e-02 -1.00944348e-01  3.40077989e-02  4.13275510e-03\n",
      " -2.93527986e-03  2.11837832e-02 -3.73962522e-02 -2.79066674e-02\n",
      " -4.61767539e-02  5.26139289e-02 -2.79734582e-02 -1.62379310e-01\n",
      "  6.61041662e-02  1.72274355e-02 -5.45110088e-03  4.74473983e-02\n",
      " -3.82237509e-02 -3.96896526e-02  1.34544959e-02  4.49653827e-02\n",
      "  4.53678053e-03  2.82978415e-02  8.36633369e-02 -1.00857848e-02\n",
      " -1.19353965e-01 -3.84624265e-02  4.82858755e-02 -9.46084335e-02\n",
      "  1.91855244e-02 -9.96518433e-02 -6.30597398e-02  3.02696396e-02\n",
      "  1.17402589e-02 -4.78372462e-02 -6.20268099e-03 -3.32850814e-02\n",
      " -4.04384080e-03  1.28307343e-02  4.05254476e-02  7.56476745e-02\n",
      "  2.92435568e-02  2.84270830e-02 -2.78938208e-02  1.66857596e-02\n",
      " -2.47961506e-02 -6.83651417e-02  2.89968904e-02 -5.39867821e-33\n",
      " -2.69011338e-03 -2.65069474e-02 -6.47918263e-04 -8.46198667e-03\n",
      " -7.35154450e-02  4.94085904e-03 -5.97842261e-02  1.03438878e-02\n",
      "  2.12898687e-03 -2.88218935e-03 -3.17076817e-02 -9.42364633e-02\n",
      "  3.03020123e-02  7.00227097e-02  4.50685173e-02  3.69439647e-02\n",
      "  1.13593601e-02  3.53026837e-02  5.50454482e-03  1.34415808e-03\n",
      "  3.46121611e-03  7.75048062e-02  5.45113087e-02 -7.92055354e-02\n",
      " -9.31696892e-02 -4.03398797e-02  3.10668685e-02 -3.83081175e-02\n",
      " -5.89442514e-02  1.93332154e-02 -2.67160498e-02 -7.91938528e-02\n",
      "  1.04206731e-04  7.70621151e-02  4.16603312e-02  8.90932307e-02\n",
      "  3.56842875e-02 -1.09153343e-02  3.71499211e-02 -2.07070690e-02\n",
      " -2.46100314e-02 -2.05025319e-02  2.62201335e-02  3.43590453e-02\n",
      "  4.39250767e-02 -8.20525829e-03 -8.40710104e-02  4.24171090e-02\n",
      "  4.87499312e-02  5.95385171e-02  2.87747644e-02  3.37638520e-02\n",
      " -4.07442674e-02 -1.66375749e-03  7.91927651e-02  3.41089033e-02\n",
      " -5.72829740e-04  1.87749639e-02 -1.36964116e-02  7.38332942e-02\n",
      "  5.74441801e-04  8.33505318e-02  5.60810938e-02 -1.13711013e-02\n",
      "  4.42611575e-02  2.69582197e-02 -4.80536409e-02 -3.15087177e-02\n",
      "  7.75226280e-02  1.81772690e-02 -8.83005112e-02 -7.85515457e-03\n",
      " -6.22243360e-02  7.19372779e-02 -2.33475324e-02  6.52480032e-03\n",
      " -9.49527323e-03 -9.88312364e-02  4.01306860e-02  3.07397302e-02\n",
      " -2.21607536e-02 -9.45910886e-02  1.02367755e-02  1.02187857e-01\n",
      " -4.12960052e-02 -3.15777920e-02  4.74751964e-02 -1.10209733e-01\n",
      "  1.69615075e-02 -3.71709578e-02 -1.03261778e-02 -4.72538806e-02\n",
      " -1.20214606e-02 -1.93254966e-02  5.79292551e-02  4.23867789e-34\n",
      "  3.92013043e-02  8.41361284e-02 -1.02946773e-01  6.92259744e-02\n",
      "  1.68820675e-02 -3.26760747e-02  9.65962932e-03  1.80899873e-02\n",
      "  2.17939764e-02  1.63188968e-02 -9.69292894e-02  3.74849676e-03\n",
      " -2.38456968e-02 -3.44056189e-02  7.11962506e-02  9.21902887e-04\n",
      " -6.23861887e-03  3.23753916e-02 -8.90376570e-04  5.01908083e-03\n",
      " -4.24537994e-02  9.89084095e-02 -4.60321046e-02  4.69704196e-02\n",
      " -1.75284538e-02 -7.02520274e-03  1.32743502e-02 -5.30152470e-02\n",
      "  2.66400818e-03  1.45819495e-02  7.43345916e-03 -3.07131726e-02\n",
      " -2.09416449e-02  8.24110433e-02 -5.15894033e-02 -2.71178540e-02\n",
      "  1.17583044e-01  7.72502460e-03 -1.89522952e-02  3.94559652e-02\n",
      "  7.17360899e-02  2.59116869e-02  2.75191832e-02  9.50549543e-03\n",
      " -3.02355122e-02 -4.07944359e-02 -1.04028441e-01 -7.97416829e-03\n",
      " -3.64458282e-03  3.29716355e-02 -2.35955026e-02 -7.50519568e-03\n",
      " -5.82234375e-02 -3.17905620e-02 -4.18049321e-02  2.17454005e-02\n",
      " -6.67292699e-02 -4.89104353e-02  4.58518090e-03 -2.66046524e-02\n",
      " -1.12597041e-01  5.11167757e-02  5.48534468e-02 -6.69857115e-02\n",
      "  1.26766279e-01 -8.59486908e-02 -5.94231971e-02 -2.92186858e-03\n",
      " -1.14875929e-02 -1.26025885e-01 -3.48287262e-03 -9.12002027e-02\n",
      " -1.22933082e-01  1.33777028e-02 -4.75775301e-02 -6.57933205e-02\n",
      " -3.39410044e-02 -3.07108276e-02 -5.22034019e-02 -2.35464349e-02\n",
      "  5.90035580e-02 -3.85757983e-02  3.19701470e-02  4.05117869e-02\n",
      "  1.67077649e-02 -3.58280949e-02  1.45688280e-02  3.20137665e-02\n",
      " -1.34843849e-02  6.07819892e-02 -8.31397064e-03 -1.08105959e-02\n",
      "  4.69410308e-02  7.66134113e-02 -4.23400216e-02 -2.11963318e-08\n",
      " -7.25293159e-02 -4.20227982e-02 -6.12373948e-02  5.24666049e-02\n",
      " -1.42363682e-02  1.18487244e-02 -1.40789077e-02 -3.67530212e-02\n",
      " -4.44977582e-02 -1.15140183e-02  5.23316972e-02  2.96651796e-02\n",
      " -4.62780558e-02 -3.70892771e-02  1.89129934e-02  2.04307232e-02\n",
      " -2.24005599e-02 -1.48563031e-02 -1.79504137e-02  4.20007929e-02\n",
      "  1.40942866e-02 -2.83492748e-02 -1.16863050e-01  1.48957139e-02\n",
      " -7.30580767e-04  5.66028282e-02 -2.68740114e-02  1.09106719e-01\n",
      "  2.94562918e-03  1.19267896e-01  1.14212409e-01  8.92973542e-02\n",
      " -1.70255713e-02 -4.99053970e-02 -2.11930834e-02  3.18421610e-02\n",
      "  7.03436360e-02 -1.02929443e-01  8.23817253e-02  2.81968247e-02\n",
      "  3.21146064e-02  3.79108414e-02 -1.09553151e-01  8.19620714e-02\n",
      "  8.73216689e-02 -5.73563837e-02 -2.01710146e-02 -5.69444522e-02\n",
      " -1.30338483e-02 -5.55684604e-02 -1.32966442e-02  8.64011329e-03\n",
      "  5.30011877e-02 -4.06846851e-02  2.71709301e-02 -2.55951099e-03\n",
      "  3.05775218e-02 -4.61865626e-02  4.68033925e-03 -3.64947282e-02\n",
      "  6.80802986e-02  6.65087402e-02  8.49152058e-02 -3.32849361e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 4.39334996e-02  5.89344166e-02  4.81783748e-02  7.75481090e-02\n",
      "  2.67443527e-02 -3.76296081e-02 -2.60516908e-03 -5.99429943e-02\n",
      " -2.49607395e-03  2.20728461e-02  4.80259173e-02  5.57553135e-02\n",
      " -3.89453806e-02 -2.66168434e-02  7.69346440e-03 -2.62376498e-02\n",
      " -3.64161246e-02 -3.78161594e-02  7.40781575e-02 -4.95050438e-02\n",
      " -5.85216656e-02 -6.36197105e-02  3.24349254e-02  2.20085159e-02\n",
      " -7.10637718e-02 -3.31577435e-02 -6.94104284e-02 -5.00374921e-02\n",
      "  7.46267810e-02 -1.11133769e-01 -1.23063475e-02  3.77455913e-02\n",
      " -2.80313343e-02  1.45353796e-02 -3.15585397e-02 -8.05835798e-02\n",
      "  5.83525822e-02  2.59009842e-03  3.92801985e-02  2.57695243e-02\n",
      "  4.98505384e-02 -1.75618543e-03 -4.55297828e-02  2.92607937e-02\n",
      " -1.02017224e-01  5.22287153e-02 -7.90899992e-02 -1.02857715e-02\n",
      "  9.20243561e-03  1.30732739e-02 -4.04778086e-02 -2.77925581e-02\n",
      "  1.24667967e-02  6.72834292e-02  6.81247041e-02 -7.57124089e-03\n",
      " -6.09944435e-03 -4.23776917e-02  5.17815053e-02 -1.56707298e-02\n",
      "  9.56355222e-03  4.12390754e-02  2.14959625e-02  1.04293441e-02\n",
      "  2.73348764e-02  1.87061653e-02 -2.69607268e-02 -7.00541511e-02\n",
      " -1.04700506e-01 -1.89870666e-03  1.77015979e-02 -5.74725270e-02\n",
      " -1.44223161e-02  4.70451836e-04  2.33230833e-03 -2.51920540e-02\n",
      "  4.93003875e-02 -5.09609841e-02  6.31983280e-02  1.49165327e-02\n",
      " -2.70766579e-02 -4.52875644e-02 -4.90594395e-02  3.74940708e-02\n",
      "  3.84580232e-02  1.56905723e-03  3.09922509e-02  2.01630369e-02\n",
      " -1.24363070e-02 -3.06719486e-02 -2.78818738e-02 -6.89182580e-02\n",
      " -5.13677187e-02  2.14796010e-02  1.15746455e-02  1.25409907e-03\n",
      "  1.88765861e-02 -4.42318656e-02 -4.49817218e-02 -3.41873220e-03\n",
      "  1.31131168e-02  2.00099573e-02  1.21099748e-01  2.31075566e-02\n",
      " -2.20159553e-02 -3.28846723e-02 -3.15516652e-03  1.17869611e-04\n",
      "  9.91498753e-02  1.65239237e-02 -4.69666533e-03 -1.45367309e-02\n",
      " -3.71074071e-03  9.65135619e-02  2.85908096e-02  2.13482324e-02\n",
      " -7.17645958e-02 -2.41141766e-02 -4.40940969e-02 -1.07346892e-01\n",
      "  6.79945499e-02  1.30466759e-01 -7.97029957e-02  6.79510459e-03\n",
      " -2.37511173e-02 -4.61636446e-02 -2.99650729e-02 -3.69410083e-33\n",
      "  7.30969384e-02 -2.20171288e-02 -8.61464664e-02 -7.14379176e-02\n",
      " -6.36741892e-02 -7.21863434e-02 -5.93038416e-03 -2.33641509e-02\n",
      " -2.83658206e-02  4.77435477e-02 -8.06176364e-02 -1.56470994e-03\n",
      "  1.38443625e-02 -2.86236238e-02 -3.35387066e-02 -1.13777541e-01\n",
      " -9.17643681e-03 -1.08100921e-02  3.23195942e-02  5.88380359e-02\n",
      "  3.34209315e-02  1.07987888e-01 -3.72712724e-02 -2.96770688e-02\n",
      "  5.17190434e-02 -2.25338619e-02 -6.96091130e-02 -2.14475449e-02\n",
      " -2.33410951e-02  4.82199676e-02 -3.58766206e-02 -4.68990654e-02\n",
      " -3.97873707e-02  1.10813215e-01 -1.43007720e-02 -1.18464492e-01\n",
      "  5.82915172e-02 -6.25889525e-02 -2.94040609e-02  6.03238717e-02\n",
      " -2.44417461e-03  1.60116162e-02  2.67234035e-02  2.49530636e-02\n",
      " -6.49318919e-02 -1.06802098e-02  2.81465594e-02  1.03563713e-02\n",
      " -6.63634564e-04  1.98185798e-02 -3.04288901e-02  6.28420059e-03\n",
      "  5.15267998e-02 -4.75375205e-02 -6.44421354e-02  9.55031887e-02\n",
      "  7.55858347e-02 -2.81574745e-02 -3.49965990e-02  1.01816416e-01\n",
      "  1.98732764e-02 -3.68037038e-02  2.93527404e-03 -5.00745662e-02\n",
      "  1.50932118e-01 -6.16080277e-02 -8.58812928e-02  7.13993656e-03\n",
      " -1.33064892e-02  7.80404955e-02  1.75250508e-02  4.21279483e-02\n",
      "  3.57939713e-02 -1.32950380e-01  3.56970876e-02 -2.03116667e-02\n",
      "  1.24910036e-02 -3.80355529e-02  4.91543151e-02 -1.56540945e-02\n",
      "  1.21418260e-01 -8.08644444e-02 -4.68782112e-02  4.10842970e-02\n",
      " -1.84318610e-02  6.69690892e-02  4.33589239e-03  2.27315500e-02\n",
      " -1.36429342e-02 -4.53238934e-02 -3.92829552e-02 -6.29893690e-03\n",
      "  5.29610030e-02 -3.69064659e-02  7.11677000e-02  2.33343214e-33\n",
      "  1.05231427e-01 -4.81874123e-02  6.95919767e-02  6.56975880e-02\n",
      " -4.65149134e-02  5.14492020e-02 -1.24476096e-02  3.20871919e-02\n",
      " -9.23356712e-02  5.00932708e-02 -3.28876860e-02  1.39138922e-02\n",
      " -8.70240678e-04 -4.90907906e-03  1.03946373e-01  3.21634783e-04\n",
      "  5.28110676e-02 -1.17990365e-02  2.31565423e-02  1.31767532e-02\n",
      " -5.25962673e-02  3.26701961e-02  3.08714109e-04  6.41128793e-02\n",
      "  3.88500877e-02  5.88008612e-02  8.29793364e-02 -1.88150015e-02\n",
      " -2.26376727e-02 -1.00473657e-01 -3.83752733e-02 -5.88081330e-02\n",
      "  1.82425312e-03 -4.26995233e-02  2.50195079e-02  6.40060157e-02\n",
      " -3.77482586e-02 -6.83900481e-03 -2.54601007e-03 -9.76043195e-02\n",
      "  1.88475959e-02 -8.83190660e-04  1.73611995e-02  7.10790679e-02\n",
      "  3.30393165e-02  6.93421625e-03 -5.60523644e-02  5.14634699e-02\n",
      " -4.29542065e-02  4.60076965e-02 -8.78830440e-03  3.17289233e-02\n",
      "  4.93965596e-02  2.95190532e-02 -5.05192243e-02 -5.43186814e-02\n",
      "  1.49937070e-04 -2.76614428e-02  3.46878842e-02 -2.10890174e-02\n",
      "  1.38060413e-02  2.99887080e-02  1.39744692e-02 -4.26472770e-03\n",
      " -1.50337499e-02 -8.76095816e-02 -6.85053766e-02 -4.28141654e-02\n",
      "  7.76945129e-02 -7.10286051e-02 -7.37695862e-03  2.13726591e-02\n",
      "  1.35562327e-02 -7.90464580e-02  5.47670061e-03  8.30663219e-02\n",
      "  1.14148088e-01  1.80757605e-03  8.75490829e-02 -4.16045450e-02\n",
      "  1.55416094e-02 -1.01206107e-02 -7.32436683e-03  1.07966410e-02\n",
      " -6.62816465e-02  3.98414135e-02 -1.16711564e-01  6.42993525e-02\n",
      "  4.02919836e-02 -6.54741079e-02  1.95052531e-02  8.09996724e-02\n",
      "  5.36463298e-02  7.67969638e-02 -1.34851858e-02 -1.76919084e-08\n",
      " -4.43934910e-02  9.20640491e-03 -8.79591033e-02  4.26921844e-02\n",
      "  7.31365532e-02  1.68427210e-02 -4.03263047e-02  1.85131654e-02\n",
      "  8.44172090e-02 -3.74477506e-02  3.02996673e-02  2.90641729e-02\n",
      "  6.36878908e-02  2.89750397e-02 -1.47270067e-02  1.77542884e-02\n",
      " -3.36896032e-02  1.73161589e-02  3.37875225e-02  1.76826194e-01\n",
      " -1.75533872e-02 -6.03077710e-02 -1.43394331e-02 -2.38536485e-02\n",
      " -4.45530713e-02 -2.89849993e-02 -8.96776989e-02 -1.75941666e-03\n",
      " -2.61486247e-02  5.94000751e-03 -5.18355444e-02  8.57279897e-02\n",
      " -8.18398148e-02  8.35442729e-03  4.00790162e-02  4.17764187e-02\n",
      "  1.04573533e-01 -2.86568422e-03  1.96690299e-02  5.81048615e-03\n",
      "  1.33253643e-02  4.51001115e-02 -2.17587780e-02 -1.39492946e-02\n",
      " -6.86992258e-02 -2.94111762e-03 -3.10765263e-02 -1.05854422e-01\n",
      "  6.91624582e-02 -4.24114503e-02 -4.67682332e-02 -3.64751294e-02\n",
      "  4.50400189e-02  6.09816574e-02 -6.56561553e-02 -5.45637030e-03\n",
      " -1.86227094e-02 -6.31483942e-02 -3.87436897e-02  3.46734524e-02\n",
      "  5.55458143e-02  5.21627367e-02  5.61065264e-02  1.02063954e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
